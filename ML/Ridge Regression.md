Here's your full **Markdown note on Ridge Regression**, complete with:

- âœ… LaTeX math handling (inline & block)
    
- âœ… Ridge regression use case (with `RMSE` & `RÂ²` before & after)
    
- âœ… Python code for implementation
    

Perfect for use in **Obsidian**, **Jupyter**, or any markdown-supporting tool.



# ðŸ“˜ Ridge Regression

## ðŸ“Œ Overview

**Ridge Regression** is a type of linear regression that includes **L2 regularization** to prevent overfitting. It shrinks coefficients by adding a penalty equivalent to the **square of the magnitude of coefficients**.

---

## ðŸ§  Mathematical Representation

### ðŸ”¹ Hypothesis Function


$\hat{y} = X\beta + \epsilon$

Where:
- \( \hat{y} \): predicted values
- \( X \): feature matrix
- \( \beta \): coefficients
- \( \epsilon \): error term

---

### ðŸ”¹ Ridge Cost Function


$J(\beta) = \sum (y_i - \hat{y}_i)^2 + \lambda \sum \beta_j^2$



Where:
- \( \lambda \): regularization strength
- \( \beta_j \): model coefficients

As \( \lambda \uparrow \), more penalty is applied, shrinking coefficients.

---

## âš™ï¸ Python Implementation (with RMSE and RÂ²)

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Load dataset
data = load_diabetes()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Split into train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ===== Linear Regression (baseline) =====
lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)

print("Linear Regression:")
print("  RMSE:", round(rmse_lr, 2))
print("  RÂ² Score:", round(r2_lr, 4))

# ===== Ridge Regression (regularized) =====
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

y_pred_ridge = ridge.predict(X_test)
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))
r2_ridge = r2_score(y_test, y_pred_ridge)

print("\nRidge Regression:")
print("  RMSE:", round(rmse_ridge, 2))
print("  RÂ² Score:", round(r2_ridge, 4))
````

---

## ðŸ“Š Output Example

```text
Linear Regression:
  RMSE: 54.23
  RÂ² Score: 0.4526

Ridge Regression:
  RMSE: 52.89
  RÂ² Score: 0.4712
```

---

## âœ… When to Use Ridge?

- Large number of features
    
- Multicollinearity present
    
- Want to **penalize large coefficients**
    
- Need a more **robust** linear model
    

---

## âš ï¸ Notes

- Ridge does **not** perform feature selection (coefficients shrink but don't become exactly 0).
    
- If you want sparsity (some features removed), consider **Lasso Regression**


## Ridge Regression from scratch
```python
class MeraRidge:
    
    def __init__(self,alpha=0.1):
        self.alpha = alpha
        self.m = None
        self.b = None
        
    def fit(self,X_train,y_train):
        
        num = 0
        den = 0
        
        for i in range(X_train.shape[0]):
            num = num + (y_train[i] - y_train.mean())*(X_train[i] - X_train.mean())
            den = den + (X_train[i] - X_train.mean())*(X_train[i] - X_train.mean())
        
        self.m = num/(den + self.alpha)
        self.b = y_train.mean() - (self.m*X_train.mean())
        print(self.m,self.b)
    %% same logif for predict with updated m as in linear regression %%
    def predict(X_test):
        pass
```

## ridge regression (nd)
```python
class MeraRidge:
    
    def __init__(self, alpha=0.1):
        # Regularization strength
        self.alpha = alpha
        
        # Model parameters (to be learned)
        self.coef_ = None
        self.intercept_ = None
        
    def fit(self, X_train, y_train):
        # Add bias (intercept) column: prepend a column of 1s
        X_train = np.insert(X_train, 0, 1, axis=1)
        
        # Identity matrix for regularization (same size as number of features + 1)
        I = np.identity(X_train.shape[1])
        
        # Do not regularize the intercept term (top-left corner set to 0)
        I[0][0] = 0
        
        # Ridge formula: (X^T X + Î±I)^(-1) X^T y
        result = np.linalg.inv(np.dot(X_train.T, X_train) + self.alpha * I).dot(X_train.T).dot(y_train)
        
        # First value is intercept (bias)
        self.intercept_ = result[0]
        
        # Remaining values are the coefficients
        self.coef_ = result[1:]
    
    def predict(self, X_test):
        # Prediction: dot product of features and coefficients + intercept
        return np.dot(X_test, self.coef_) + self.intercept_

```

## Ridge regression using GD
```PYTHON
class MeraRidgeGD:
    
    def __init__(self, epochs, learning_rate, alpha):
        # Number of iterations for gradient descent
        self.epochs = epochs
        
        # Step size for gradient update
        self.learning_rate = learning_rate
        
        # Regularization strength (Î»)
        self.alpha = alpha
        
        # Model parameters to be learned
        self.coef_ = None
        self.intercept_ = None

    def fit(self, X_train, y_train):
        # Initialize weights (coefficients) with ones
        self.coef_ = np.ones(X_train.shape[1])
        
        # Initialize intercept (bias) to 0
        self.intercept_ = 0

        # Combine intercept and weights into a single vector 'theta'
        theta = np.insert(self.coef_, 0, self.intercept_)

        # Add a column of 1s to X_train to account for intercept
        X_train = np.insert(X_train, 0, 1, axis=1)

        # Perform gradient descent for the specified number of epochs
        for i in range(self.epochs):
            # Compute gradient of loss function with respect to theta
            # Gradient: âˆ‡ = (Xáµ€X)Î¸ - Xáµ€y + Î»Î¸
            theta_derivative = np.dot(X_train.T, X_train).dot(theta) - np.dot(X_train.T, y_train) + self.alpha * theta

            # Update theta using gradient descent
            theta = theta - self.learning_rate * theta_derivative

        # Extract learned weights and intercept from final theta
        self.coef_ = theta[1:]           # Skip first term (intercept)
        self.intercept_ = theta[0]       # First term is intercept

    def predict(self, X_test):
        # Predict using learned weights and bias
        return np.dot(X_test, self.coef_) + self.intercept_

```



# what happens  to coff of linear regression when ridge regression is applied .
| **Î± (alpha)**     | **Effect on Coefficients (`w`)**                                                                                                                 | **Explanation**                                                            |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------- |
| **Î± = 0**         | Equivalent to **OLS Linear Regression** (no regularization). Coefficients are unregularized and can be large if data is multicollinear or noisy. |                                                                            |
| **Î± > 0 (small)** | Coefficients are slightly shrunk toward 0, but still close to OLS values.                                                                        | Regularization begins to penalize large weights, improving generalization. |
| **Î± increases**   | Coefficients shrink more. Some become very small but never exactly zero.                                                                         | The penalty term `Î± * Î£(wáµ¢Â²)` increases, discouraging large coefficients.  |
| **Î± â†’ âˆž**         | All coefficients â†’ 0                                                                                                                             | The model underfits badly. Predictions approach the mean of `y`.           |
### ðŸ” Summary of Effects:

- **Variance â†“** (model becomes more stable)
    
- **Bias â†‘** (model becomes simpler, may underfit)
    
- **Overfitting â†“**
    
- **Coefficients Magnitude â†“**
# higher values are impacted more?
Ridge regression adds a **penalty proportional to the square of each coefficient**:
 $$J(w)=RSS+Î±j=1âˆ‘pâ€‹wj2â€‹$$
 This means:

- The **penalty grows quadratically** with the size of each coefficient.
    
- So, **larger coefficients contribute disproportionately** to the penalty term.
    
- To minimize the cost, the optimizer will **shrink larger coefficients more aggressively**.

### ðŸ” Example:

If you have:

- `wâ‚ = 10` â†’ penalty = 100
    
- `wâ‚‚ = 2` â†’ penalty = 4
    

Then increasing `Î±` pushes the optimizer to reduce `wâ‚` much more than `wâ‚‚`, since it's costing more in the loss function.

---

### ðŸ§  Intuition:

Ridge regression discourages complexity. Large weights often suggest reliance on certain features too heavily. So Ridge "balances" the model by **flattening** out the influence of dominant features, helping generalization.

# how bias variance tradeoff is impacted?
### ðŸ”„ Effect on Bias-Variance Tradeoff as Î± â†‘:

| **As Î± increases**   | **Bias**                          | **Variance**                             | **Overall Effect**                              |
| -------------------- | --------------------------------- | ---------------------------------------- | ----------------------------------------------- |
| â¬†ï¸ Increases         | â¬†ï¸ Increases (model gets simpler) | â¬‡ï¸ Decreases (model becomes more stable) | Helps reduce overfitting but may underfit       |
| â¬‡ï¸ Decreases (Î± â†’ 0) | â¬‡ï¸ Bias decreases                 | â¬†ï¸ Variance increases                    | Model fits training data better but may overfit |
# what happens to loss function?
$$J(w)=ResidualÂ SumÂ ofÂ SquaresÂ (RSS)i=1âˆ‘nâ€‹(yiâ€‹âˆ’y^â€‹iâ€‹)2â€‹â€‹+L2Â RegularizationÂ PenaltyÎ±j=1âˆ‘pâ€‹wj2â€‹â€‹â€‹$$
- The first term (**RSS**) measures how well the model fits the data.
    
- The second term (**L2 penalty**) penalizes large weights.
    
- **Î± controls the tradeoff** between the two.
### ðŸ” As Î± Increases:

|**Component**|**Effect**|
|---|---|
|**L2 Penalty (Î±â€†Î£â€†wÂ²)**|Increases â€” it weighs more heavily in the total loss|
|**RSS (Data Fit)**|Gets less priority â€” the model is allowed to fit the data less accurately to keep weights small|
|**Total Loss (J)**|May increase or decrease â€” model tries to minimize a new balance between fit and simplicity|

---

### ðŸ§  Intuition:

- When **Î± = 0** â†’ No regularization â†’ Model purely minimizes RSS â†’ can overfit.
    
- When **Î± increases** â†’ Model sacrifices data fit (higher RSS) to reduce complexity (smaller weights) â†’ better generalization.
    
- When **Î± â†’ âˆž** â†’ Model minimizes only the weight size â†’ all weights shrink toward 0 â†’ predictions become constant (underfitting).
    

---

So, Ridge **reshapes the landscape of the loss function** â€” the minimum shifts from best-fit to best-regularized fit.

# why its called ridge?
### ðŸ§  Why is it called **Ridge Regression**?

- Imagine you're trying to find the lowest point in a valley.
    
- In regular linear regression, if the valley floor is **flat and long** (due to correlated features), itâ€™s hard to know where the exact lowest point is â€” your solution could slide around. This flat area is like a **ridge** (a long, narrow elevated region).
    
- **Ridge Regression** fixes this by adding a rule:  
    ðŸ‘‰ _â€œDonâ€™t let the model coefficients get too big.â€_  
    This **raises the flat valley into a peak**, helping you find **one clear best solution**.
    

---

### ðŸ” In short:

> It's called **Ridge** because it deals with the **ridge-shaped, unstable areas** in regular regression by **penalizing large coefficients**, leading to a **more stable solution**.