# ğŸ“˜ Multiple Linear Regression (MLR)

## ğŸ“Œ What is Multiple Linear Regression?

Multiple Linear Regression (MLR) is a **supervised learning** technique used to model the relationship between **one dependent variable** and **multiple independent variables**.

Itâ€™s an extension of **Simple Linear Regression**, which only uses one independent variable.

---

## ğŸ§® Regression Equation

The general form of the MLR equation is:

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n + \varepsilon
$$

Where:
- $y$: Dependent variable (target)
- $x_1, x_2, \dots, x_n$: Independent variables (features)
- $\beta_0$: Intercept
- $\beta_1, \beta_2, \dots, \beta_n$: Coefficients
- $\varepsilon$: Error term

---

## ğŸ” Assumptions of MLR

1. **Linearity**: The relationship between dependent and independent variables is linear.
2. **Independence**: Observations are independent.
3. **Homoscedasticity**: Constant variance of the error terms.
4. **Normality**: Error terms are normally distributed.
5. **No Multicollinearity**: Independent variables are not highly correlated.

---

## ğŸ§  Interpretation of Coefficients

Each coefficient $\beta_j$ represents the change in the target variable $y$ for a **one-unit increase in** $x_j$, holding all other variables constant.

---

## ğŸ§ª Cost Function (Least Squares)

The cost function used in MLR is the **Sum of Squared Errors (SSE)**:

$$
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Where:
- $\hat{y}_i = \beta_0 + \beta_1x_{i1} + \cdots + \beta_nx_{in}$  
- Goal: Minimize $J(\beta)$

---

## ğŸ“Š Evaluation Metrics

### ğŸ”¹ Mean Absolute Error (MAE)

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

### ğŸ”¹ Mean Squared Error (MSE)

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

### ğŸ”¹ Root Mean Squared Error (RMSE)

$$
\text{RMSE} = \sqrt{\text{MSE}}
$$

### ğŸ”¹ $R^2$ Score (Coefficient of Determination)

$$
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
$$

### ğŸ”¹ Adjusted $R^2$

Adjusted $R^2$ accounts for the number of features:

$$
\text{Adjusted } R^2 = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - k - 1} \right)
$$

Where:
- $n$: Number of observations
- $k$: Number of predictors

---

## ğŸ’» Python Implementation


```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("MAE:", mean_absolute_error(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))
print("R2:", r2_score(y_test, y_pred))
```

##  FROM SCRATCH
```Python
class MeraLR:

Â  Â  def __init__(self):

Â  Â  Â  Â  self.coef_ = None

Â  Â  Â  Â  self.intercept_ = None

Â  Â  def fit(self,X_train,y_train):

Â  Â  Â  Â  X_train = np.insert(X_train,0,1,axis=1)

Â  Â  Â  Â  # calcuate the coeffs

Â  Â  Â  Â  betas = np.linalg.inv(np.dot(X_train.T,X_train)).dot(X_train.T).dot(y_train)

Â  Â  Â  Â  self.intercept_ = betas[0]

Â  Â  Â  Â  self.coef_ = betas[1:]

Â  Â  def predict(self,X_test):

Â  Â  Â  Â  y_pred = np.dot(X_test,self.coef_) + self.intercept_

Â  Â  Â  Â  return y_pred
```
